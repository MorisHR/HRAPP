# ============================================================================
# Fortune 500 Monitoring Stack - Docker Compose
# Handles: Millions of requests per minute
# Components: Prometheus, Grafana, Alertmanager, Exporters
# Performance: Optimized for high-throughput multi-tenant SaaS
# ============================================================================

version: '3.8'

networks:
  monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  alertmanager_data:
    driver: local

services:
  # ==========================================================================
  # Prometheus - Time Series Database & Metrics Collection
  # Performance: 1M+ samples/sec ingestion rate
  # Storage: Optimized for high cardinality metrics
  # ==========================================================================
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: hrms-prometheus
    restart: unless-stopped
    user: root
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=90d'
      - '--storage.tsdb.retention.size=50GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      # HIGH PERFORMANCE SETTINGS for millions of req/min
      - '--storage.tsdb.min-block-duration=2h'
      - '--storage.tsdb.max-block-duration=2h'
      - '--query.max-concurrency=50'
      - '--query.timeout=2m'
      - '--query.max-samples=50000000'
      # Memory optimization
      - '--storage.tsdb.wal-compression'
      # Remote write buffer (for high ingestion)
      - '--storage.remote.flush-deadline=5m'
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/alerts:/etc/prometheus/alerts:ro
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      monitoring:
        ipv4_address: 172.28.0.10
    environment:
      - TZ=UTC
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      com.hrms.service: "prometheus"
      com.hrms.tier: "monitoring"

  # ==========================================================================
  # Alertmanager - Alert Routing & Notification
  # Handles: Thousands of alerts/min
  # Deduplication: Prevents alert storms
  # ==========================================================================
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: hrms-alertmanager
    restart: unless-stopped
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
      - '--cluster.advertise-address=0.0.0.0:9093'
      # High availability settings
      - '--log.level=info'
    volumes:
      - ./prometheus/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    ports:
      - "9093:9093"
    networks:
      monitoring:
        ipv4_address: 172.28.0.11
    environment:
      - TZ=UTC
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      com.hrms.service: "alertmanager"
      com.hrms.tier: "monitoring"

  # ==========================================================================
  # Grafana - Visualization & Dashboards
  # Performance: Sub-second dashboard load times
  # Features: Auto-provisioning of dashboards & datasources
  # ==========================================================================
  grafana:
    image: grafana/grafana:10.1.5
    container_name: hrms-grafana
    restart: unless-stopped
    user: root
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      monitoring:
        ipv4_address: 172.28.0.12
    environment:
      # Admin credentials
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=HRMSAdmin2025!
      # Performance settings
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      # Database backend (for high availability)
      - GF_DATABASE_TYPE=sqlite3
      - GF_DATABASE_PATH=/var/lib/grafana/grafana.db
      # Server settings
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_SERVER_DOMAIN=localhost
      # Security
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_AUTH_ANONYMOUS_ENABLED=false
      # Feature toggles
      - GF_FEATURE_TOGGLES_ENABLE=publicDashboards
      # Rendering (for PDF exports)
      - GF_RENDERING_SERVER_URL=http://renderer:8081/render
      - GF_RENDERING_CALLBACK_URL=http://grafana:3000/
      # Performance
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=5s
      - GF_DATAPROXY_TIMEOUT=300
      - GF_DATAPROXY_KEEP_ALIVE_SECONDS=300
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      com.hrms.service: "grafana"
      com.hrms.tier: "monitoring"
    depends_on:
      - prometheus

  # ==========================================================================
  # PostgreSQL Exporter - Database Metrics
  # Performance: <5ms query overhead
  # Metrics: 100+ PostgreSQL performance indicators
  # ==========================================================================
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: hrms-postgres-exporter
    restart: unless-stopped
    ports:
      - "9187:9187"
    networks:
      monitoring:
        ipv4_address: 172.28.0.13
    environment:
      # Connection to host PostgreSQL (adjust for your setup)
      - DATA_SOURCE_NAME=postgresql://postgres:postgres@host.docker.internal:5432/hrms_master?sslmode=disable
      # Metrics configuration
      - PG_EXPORTER_EXTEND_QUERY_PATH=/etc/postgres_exporter/queries.yaml
      - PG_EXPORTER_DISABLE_DEFAULT_METRICS=false
      - PG_EXPORTER_DISABLE_SETTINGS_METRICS=false
      - PG_EXPORTER_AUTO_DISCOVER_DATABASES=true
      # Performance tuning
      - PG_EXPORTER_METRIC_PREFIX=pg
      - PG_EXPORTER_CONSTANT_LABELS=environment=production,cluster=primary
    volumes:
      - ./prometheus/postgres-exporter-queries.yaml:/etc/postgres_exporter/queries.yaml:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9187/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    extra_hosts:
      - "host.docker.internal:host-gateway"
    labels:
      com.hrms.service: "postgres-exporter"
      com.hrms.tier: "monitoring"

  # ==========================================================================
  # Node Exporter - System Metrics (CPU, Memory, Disk, Network)
  # Performance: Minimal overhead (<0.1% CPU)
  # ==========================================================================
  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: hrms-node-exporter
    restart: unless-stopped
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      - '--collector.netclass.ignored-devices=^(veth.*|docker.*|br-.*)$$'
      - '--collector.netdev.device-exclude=^(veth.*|docker.*|br-.*)$$'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    ports:
      - "9100:9100"
    networks:
      monitoring:
        ipv4_address: 172.28.0.14
    labels:
      com.hrms.service: "node-exporter"
      com.hrms.tier: "monitoring"

  # ==========================================================================
  # Redis Exporter - Cache Metrics
  # Monitors: Redis performance for token blacklist, rate limiting
  # ==========================================================================
  redis-exporter:
    image: oliver006/redis_exporter:v1.55.0
    container_name: hrms-redis-exporter
    restart: unless-stopped
    ports:
      - "9121:9121"
    networks:
      monitoring:
        ipv4_address: 172.28.0.15
    environment:
      - REDIS_ADDR=host.docker.internal:6379
      - REDIS_PASSWORD=
      - REDIS_EXPORTER_CHECK_KEYS=hrms:*
      - REDIS_EXPORTER_CHECK_SINGLE_KEYS=hrms:blacklist:*,hrms:ratelimit:*
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9121/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      com.hrms.service: "redis-exporter"
      com.hrms.tier: "monitoring"

  # ==========================================================================
  # Custom HRMS Metrics Exporter
  # Exposes: Application-specific business metrics from monitoring schema
  # Performance: Query-optimized with materialized views
  # ==========================================================================
  hrms-metrics-exporter:
    build:
      context: ./exporters/hrms-exporter
      dockerfile: Dockerfile
    container_name: hrms-metrics-exporter
    restart: unless-stopped
    ports:
      - "9188:9188"
    networks:
      monitoring:
        ipv4_address: 172.28.0.16
    environment:
      - DATABASE_URL=postgresql://monitoring_reader:monitoring_pass@host.docker.internal:5432/hrms_master
      - EXPORTER_PORT=9188
      - METRICS_REFRESH_INTERVAL=30
      - LOG_LEVEL=info
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9188/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      com.hrms.service: "hrms-metrics-exporter"
      com.hrms.tier: "monitoring"

  # ==========================================================================
  # Grafana Renderer - PDF Export Service
  # Enables: Automated report generation and PDF exports
  # ==========================================================================
  renderer:
    image: grafana/grafana-image-renderer:3.8.3
    container_name: hrms-grafana-renderer
    restart: unless-stopped
    ports:
      - "8081:8081"
    networks:
      monitoring:
        ipv4_address: 172.28.0.17
    environment:
      - ENABLE_METRICS=true
      - LOG_LEVEL=info
      - RENDERING_MODE=clustered
      - RENDERING_CLUSTERING_MODE=context
      - RENDERING_CLUSTERING_MAX_CONCURRENCY=10
    labels:
      com.hrms.service: "grafana-renderer"
      com.hrms.tier: "monitoring"

  # ==========================================================================
  # Loki - Log Aggregation (Optional but recommended)
  # Handles: Application logs alongside metrics
  # Performance: High-throughput log ingestion
  # ==========================================================================
  loki:
    image: grafana/loki:2.9.2
    container_name: hrms-loki
    restart: unless-stopped
    ports:
      - "3100:3100"
    volumes:
      - ./loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
    networks:
      monitoring:
        ipv4_address: 172.28.0.18
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      com.hrms.service: "loki"
      com.hrms.tier: "monitoring"

  # ==========================================================================
  # Promtail - Log Shipper (sends logs to Loki)
  # Monitors: Application logs, API logs, error logs
  # ==========================================================================
  promtail:
    image: grafana/promtail:2.9.2
    container_name: hrms-promtail
    restart: unless-stopped
    volumes:
      - ./loki/promtail-config.yaml:/etc/promtail/config.yaml:ro
      - /var/log/hrms:/var/log/hrms:ro
      - /workspaces/HRAPP/src/HRMS.API/Logs:/var/log/api:ro
    networks:
      monitoring:
        ipv4_address: 172.28.0.19
    command: -config.file=/etc/promtail/config.yaml
    labels:
      com.hrms.service: "promtail"
      com.hrms.tier: "monitoring"
    depends_on:
      - loki

# ==========================================================================
# Health Check Service - Monitors the monitoring stack itself
# Ensures: All exporters and services are healthy
# ==========================================================================
  healthcheck:
    image: alpine:3.18
    container_name: hrms-monitoring-healthcheck
    restart: unless-stopped
    networks:
      - monitoring
    volumes:
      - ./scripts/healthcheck.sh:/healthcheck.sh:ro
    command:
      - /bin/sh
      - -c
      - |
        while true; do
          sh /healthcheck.sh
          sleep 60
        done
    labels:
      com.hrms.service: "healthcheck"
      com.hrms.tier: "monitoring"
